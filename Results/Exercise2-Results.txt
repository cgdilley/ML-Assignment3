
================================================================================================================================

For exercise 2, I attempted two different approaches.  First, I tried using the same vectors as in exercise 1, except instead of relative frequencies, I binarized the vectors to be one-hot representations of all possible unigrams/bigrams.  Doing so, I achieved exemplary results.  Across 10 folds (with 2 training epochs before testing for each fold), I achieved an average accuracy of 98.65% and 98.5% for unigrams and bigrams respectively.  After only a few epochs, the model was trained to have essentially 100% accuracy, with a calculated binary cross-entropy loss value of less than 10^-6.

The sizes of the input vectors were again 15283 and 40864 for unigrams and bigrams respectively.  I also utilized 1024 hidden units in the multi-layer perceptron.  These together would represent the total number of weights that needed to be calculated.

The code for this method is found in the file "Ex1,2(OH).py".



The second approach utilized word vectors from GloVe, as I believe that is what was intended by the assignment for the purposes of comparison to CNNs.  Each document was represented the sum of all word vectors of words in the document (for bigrams, the word vectors of both components were concatenated).  As such, each document was represented by a 1-dimensional vector of length 100 (200 for bigrams).  With this methodolgy, I achieved rather poor results.  For unigrams I had an average accuracy of 64.3% (peaking at 74% on fold 9), and for bigrams I had an average accuracy of 67.65% (peaking at 80% on fold 9).

However, the processing was exceptionally quick.  As shown above, every epoch took under one second to perform.  As I was using then length-100 vectors from GloVe, again with 1024 hidden units, the number of weights to be calculated was significantly smaller.

The code for this method is found in the file "Ex1,2(WV).py"


================================================================================================================================
OUTPUT FROM CODE:




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%    METHOD ONE:  ONE-HOT UNIGRAM/BIGRAM VECTORS   
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

EXERCISE 2:  Multi-layer Perceptron
====================================
Binarizing predictors...
Training and testing model...
 - Unigrams:
-------------
Fold #1:
------------------
Epoch 1/2
1800/1800 [==============================] - 8s - loss: 0.5645 - acc: 0.7233     
Epoch 2/2
1800/1800 [==============================] - 8s - loss: 0.0808 - acc: 0.9789     
200/200 [==============================] - 0s     
Fold accuracy = 0.865

Fold #2:
------------------
Epoch 1/2
1800/1800 [==============================] - 9s - loss: 0.0769 - acc: 0.9756     
Epoch 2/2
1800/1800 [==============================] - 8s - loss: 0.0107 - acc: 0.9961     
200/200 [==============================] - 0s     
Fold accuracy = 1.0

Fold #3:
------------------
Epoch 1/2
1800/1800 [==============================] - 8s - loss: 0.0034 - acc: 0.9983     
Epoch 2/2
1800/1800 [==============================] - 8s - loss: 8.1654e-05 - acc: 1.0000     
200/200 [==============================] - 0s     
Fold accuracy = 1.0

Fold #4:
------------------
Epoch 1/2
1800/1800 [==============================] - 8s - loss: 0.0066 - acc: 0.9983     
Epoch 2/2
1800/1800 [==============================] - 8s - loss: 2.8293e-06 - acc: 1.0000     
200/200 [==============================] - 0s     
Fold accuracy = 1.0

Fold #5:
------------------
Epoch 1/2
1800/1800 [==============================] - 8s - loss: 6.5480e-07 - acc: 1.0000     
Epoch 2/2
1800/1800 [==============================] - 9s - loss: 2.4147e-07 - acc: 1.0000     
200/200 [==============================] - 0s     
Fold accuracy = 1.0

Fold #6:
------------------
Epoch 1/2
1800/1800 [==============================] - 9s - loss: 8.7643e-07 - acc: 1.0000     
Epoch 2/2
1800/1800 [==============================] - 9s - loss: 2.1968e-07 - acc: 1.0000     
200/200 [==============================] - 0s     
Fold accuracy = 1.0

Fold #7:
------------------
Epoch 1/2
1800/1800 [==============================] - 10s - loss: 1.3391e-07 - acc: 1.0000    
Epoch 2/2
1800/1800 [==============================] - 12s - loss: 1.2550e-07 - acc: 1.0000    
200/200 [==============================] - 0s     
Fold accuracy = 1.0

Fold #8:
------------------
Epoch 1/2
1800/1800 [==============================] - 15s - loss: 1.2100e-07 - acc: 1.0000    
Epoch 2/2
1800/1800 [==============================] - 19s - loss: 1.2007e-07 - acc: 1.0000    
200/200 [==============================] - 0s     
Fold accuracy = 1.0

Fold #9:
------------------
Epoch 1/2
1800/1800 [==============================] - 27s - loss: 1.1987e-07 - acc: 1.0000    
Epoch 2/2
1800/1800 [==============================] - 38s - loss: 1.1957e-07 - acc: 1.0000    
200/200 [==============================] - 0s     
Fold accuracy = 1.0

Fold #10:
------------------
Epoch 1/2
1800/1800 [==============================] - 46s - loss: 1.1964e-07 - acc: 1.0000    
Epoch 2/2
1800/1800 [==============================] - 50s - loss: 1.1944e-07 - acc: 1.0000    
200/200 [==============================] - 0s     
Fold accuracy = 1.0


Unigram results:
[ 0.865  1.     1.     1.     1.     1.     1.     1.     1.     1.   ]
MEAN = 0.9865

 - Bigrams:
------------
Fold #1:
------------------
Epoch 1/2
1800/1800 [==============================] - 25s - loss: 0.4841 - acc: 0.7817    
Epoch 2/2
1800/1800 [==============================] - 25s - loss: 0.0089 - acc: 1.0000    
200/200 [==============================] - 0s     
Fold accuracy = 0.85

Fold #2:
------------------
Epoch 1/2
1800/1800 [==============================] - 25s - loss: 0.0705 - acc: 0.9794    
Epoch 2/2
1800/1800 [==============================] - 25s - loss: 5.7815e-04 - acc: 1.0000    
200/200 [==============================] - 0s     
Fold accuracy = 1.0

Fold #3:
------------------
Epoch 1/2
1800/1800 [==============================] - 25s - loss: 3.0953e-04 - acc: 1.0000    
Epoch 2/2
1800/1800 [==============================] - 25s - loss: 1.6800e-05 - acc: 1.0000    
200/200 [==============================] - 0s     
Fold accuracy = 1.0

Fold #4:
------------------
Epoch 1/2
1800/1800 [==============================] - 25s - loss: 7.2116e-06 - acc: 1.0000    
Epoch 2/2
1800/1800 [==============================] - 25s - loss: 1.4945e-06 - acc: 1.0000    
200/200 [==============================] - 0s     
Fold accuracy = 1.0

Fold #5:
------------------
Epoch 1/2
1800/1800 [==============================] - 25s - loss: 1.2322e-07 - acc: 1.0000    
Epoch 2/2
1800/1800 [==============================] - 25s - loss: 1.2027e-07 - acc: 1.0000    
200/200 [==============================] - 0s     
Fold accuracy = 1.0

Fold #6:
------------------
Epoch 1/2
1800/1800 [==============================] - 26s - loss: 3.8554e-07 - acc: 1.0000    
Epoch 2/2
1800/1800 [==============================] - 29s - loss: 1.7501e-07 - acc: 1.0000    
200/200 [==============================] - 0s     
Fold accuracy = 1.0

Fold #7:
------------------
Epoch 1/2
1800/1800 [==============================] - 45s - loss: 1.3126e-07 - acc: 1.0000    
Epoch 2/2
1800/1800 [==============================] - 80s - loss: 1.2325e-07 - acc: 1.0000    
200/200 [==============================] - 0s     
Fold accuracy = 1.0

Fold #8:
------------------
Epoch 1/2
1800/1800 [==============================] - 118s - loss: 1.2093e-07 - acc: 1.0000   
Epoch 2/2
1800/1800 [==============================] - 136s - loss: 1.2027e-07 - acc: 1.0000   
200/200 [==============================] - 0s     
Fold accuracy = 1.0

Fold #9:
------------------
Epoch 1/2
1800/1800 [==============================] - 138s - loss: 1.1987e-07 - acc: 1.0000   
Epoch 2/2
1800/1800 [==============================] - 138s - loss: 1.1967e-07 - acc: 1.0000   
200/200 [==============================] - 0s     
Fold accuracy = 1.0

Fold #10:
------------------
Epoch 1/2
1800/1800 [==============================] - 138s - loss: 1.1954e-07 - acc: 1.0000   
Epoch 2/2
1800/1800 [==============================] - 138s - loss: 1.1947e-07 - acc: 1.0000   
200/200 [==============================] - 0s     
Fold accuracy = 1.0


Bigram results:
[ 0.85  1.    1.    1.    1.    1.    1.    1.    1.    1.  ]
MEAN = 0.985






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%    METHOD TWO:  GloVe WORD VECTORS 
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%







EXERCISE 2:  Multi-layer Perceptron
====================================
Converting feature vectors...
Training and testing model...
 - Unigrams:
-------------
Fold #1:
------------------
Epoch 1/2
1800/1800 [==============================] - 0s - loss: 0.6931 - acc: 0.5428     
Epoch 2/2
1800/1800 [==============================] - 0s - loss: 0.6570 - acc: 0.6022     
 32/200 [===>..........................] - ETA: 0s
Fold accuracy = 0.65

Fold #2:
------------------
Epoch 1/2
1800/1800 [==============================] - 0s - loss: 0.6432 - acc: 0.6367     
Epoch 2/2
1800/1800 [==============================] - 0s - loss: 0.6250 - acc: 0.6511     
 32/200 [===>..........................] - ETA: 0s
Fold accuracy = 0.51

Fold #3:
------------------
Epoch 1/2
1800/1800 [==============================] - 0s - loss: 0.6214 - acc: 0.6606     
Epoch 2/2
1800/1800 [==============================] - 0s - loss: 0.6072 - acc: 0.6556     
 32/200 [===>..........................] - ETA: 0s
Fold accuracy = 0.67

Fold #4:
------------------
Epoch 1/2
1800/1800 [==============================] - 0s - loss: 0.6053 - acc: 0.6756     
Epoch 2/2
1800/1800 [==============================] - 0s - loss: 0.5913 - acc: 0.6894     
 32/200 [===>..........................] - ETA: 0s
Fold accuracy = 0.565

Fold #5:
------------------
Epoch 1/2
1800/1800 [==============================] - 0s - loss: 0.5880 - acc: 0.6917     
Epoch 2/2
1800/1800 [==============================] - 0s - loss: 0.5807 - acc: 0.6900     
 32/200 [===>..........................] - ETA: 0s
Fold accuracy = 0.68

Fold #6:
------------------
Epoch 1/2
1800/1800 [==============================] - 0s - loss: 0.5689 - acc: 0.7111     
Epoch 2/2
1800/1800 [==============================] - 0s - loss: 0.5643 - acc: 0.7011     
 32/200 [===>..........................] - ETA: 0s
Fold accuracy = 0.69

Fold #7:
------------------
Epoch 1/2
1800/1800 [==============================] - 0s - loss: 0.5564 - acc: 0.7139     
Epoch 2/2
1800/1800 [==============================] - 0s - loss: 0.5672 - acc: 0.6978     
 32/200 [===>..........................] - ETA: 0s
Fold accuracy = 0.64

Fold #8:
------------------
Epoch 1/2
1800/1800 [==============================] - 0s - loss: 0.5539 - acc: 0.7094     
Epoch 2/2
1800/1800 [==============================] - 0s - loss: 0.5579 - acc: 0.7011     
 32/200 [===>..........................] - ETA: 0s
Fold accuracy = 0.55

Fold #9:
------------------
Epoch 1/2
1800/1800 [==============================] - 0s - loss: 0.5523 - acc: 0.7067     
Epoch 2/2
1800/1800 [==============================] - 0s - loss: 0.5438 - acc: 0.7150     
 32/200 [===>..........................] - ETA: 0s
Fold accuracy = 0.74

Fold #10:
------------------
Epoch 1/2
1800/1800 [==============================] - 0s - loss: 0.5392 - acc: 0.7133     
Epoch 2/2
1800/1800 [==============================] - 0s - loss: 0.5430 - acc: 0.7211     
 32/200 [===>..........................] - ETA: 0s
Fold accuracy = 0.735


Unigram results:
[ 0.65   0.51   0.67   0.565  0.68   0.69   0.64   0.55   0.74   0.735]
MEAN = 0.643

 - Bigrams:
------------
Fold #1:
------------------
Epoch 1/2
1800/1800 [==============================] - 0s - loss: 0.7039 - acc: 0.5394     
Epoch 2/2
1800/1800 [==============================] - 0s - loss: 0.6553 - acc: 0.6033     
 32/200 [===>..........................] - ETA: 0sFold accuracy = 0.58

Fold #2:
------------------
Epoch 1/2
1800/1800 [==============================] - 0s - loss: 0.6490 - acc: 0.6056     
Epoch 2/2
1800/1800 [==============================] - 0s - loss: 0.6390 - acc: 0.6406     
 32/200 [===>..........................] - ETA: 0sFold accuracy = 0.635

Fold #3:
------------------
Epoch 1/2
1800/1800 [==============================] - 0s - loss: 0.6187 - acc: 0.6517     
Epoch 2/2
1800/1800 [==============================] - 0s - loss: 0.6083 - acc: 0.6778     
 32/200 [===>..........................] - ETA: 0sFold accuracy = 0.53

Fold #4:
------------------
Epoch 1/2
1800/1800 [==============================] - 0s - loss: 0.6065 - acc: 0.6728     
Epoch 2/2
1800/1800 [==============================] - 0s - loss: 0.5901 - acc: 0.6906     
 32/200 [===>..........................] - ETA: 0sFold accuracy = 0.595

Fold #5:
------------------
Epoch 1/2
1800/1800 [==============================] - 0s - loss: 0.5954 - acc: 0.6817     
Epoch 2/2
1800/1800 [==============================] - 0s - loss: 0.5803 - acc: 0.6939     
 32/200 [===>..........................] - ETA: 0sFold accuracy = 0.755

Fold #6:
------------------
Epoch 1/2
1800/1800 [==============================] - 0s - loss: 0.5649 - acc: 0.7100     
Epoch 2/2
1800/1800 [==============================] - 0s - loss: 0.5796 - acc: 0.6872     
 32/200 [===>..........................] - ETA: 0sFold accuracy = 0.73

Fold #7:
------------------
Epoch 1/2
1800/1800 [==============================] - 0s - loss: 0.5610 - acc: 0.7078     
Epoch 2/2
1800/1800 [==============================] - 0s - loss: 0.5535 - acc: 0.7189     
 32/200 [===>..........................] - ETA: 0sFold accuracy = 0.735

Fold #8:
------------------
Epoch 1/2
1800/1800 [==============================] - 0s - loss: 0.5564 - acc: 0.7072     
Epoch 2/2
1800/1800 [==============================] - 0s - loss: 0.5471 - acc: 0.7150     
 32/200 [===>..........................] - ETA: 0sFold accuracy = 0.65

Fold #9:
------------------
Epoch 1/2
1800/1800 [==============================] - 0s - loss: 0.5505 - acc: 0.7211     
Epoch 2/2
1800/1800 [==============================] - 0s - loss: 0.5458 - acc: 0.7244     
 32/200 [===>..........................] - ETA: 0sFold accuracy = 0.8

Fold #10:
------------------
Epoch 1/2
1800/1800 [==============================] - 0s - loss: 0.5473 - acc: 0.7200     
Epoch 2/2
1800/1800 [==============================] - 0s - loss: 0.5348 - acc: 0.7261     
 32/200 [===>..........................] - ETA: 0sFold accuracy = 0.755


Bigram results:
[ 0.58   0.635  0.53   0.595  0.755  0.73   0.735  0.65   0.8    0.755]
MEAN = 0.6765




